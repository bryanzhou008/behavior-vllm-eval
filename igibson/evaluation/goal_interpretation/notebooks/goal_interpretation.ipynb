{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from enum import Enum\n",
    "import re\n",
    "import copy\n",
    "\n",
    "\n",
    "import openai\n",
    "import os\n",
    "import time\n",
    "import ast\n",
    "import tqdm\n",
    "\n",
    "def evaluate_goal_interpretation_plan_succ(data_dir, t_ids, node_goal_list, edge_goal_list, action_goals, num_tasks=50, case_path=None, save_case=False):\n",
    "    properties_data = utils.load_properties_data()\n",
    "    object_states = utils.load_object_states()\n",
    "    object_placing = utils.load_object_placing()\n",
    "    name_equivalence = utils.load_name_equivalence()\n",
    "\n",
    "    scene_id = [1]\n",
    "    program_dir = os.path.join(data_dir, 'executable_programs')\n",
    "    all_rel_path = '/viscam/u/shiyuz/svl_project/AgentEval/virtualhome/resources/relation_types.json'\n",
    "    all_action_path = '/viscam/u/shiyuz/svl_project/AgentEval/virtualhome/resources/action_space.json'\n",
    "    with open(all_rel_path, 'r') as f:\n",
    "        all_rel = json.load(f)\n",
    "    with open(all_action_path, 'r') as f:\n",
    "        action_space = json.load(f)\n",
    "\n",
    "    tot_num = 0.0\n",
    "    tot_nodes = 0.0\n",
    "    tot_edges = 0.0\n",
    "    tot_actions = 0.0\n",
    "    tot_succ = 0.0\n",
    "    succ_nodes = 0.0\n",
    "    succ_edges = 0.0\n",
    "    succ_actions = 0.0\n",
    "    pattern = r\"file(\\d+_\\d+)\\.txt\"\n",
    "\n",
    "    for scene in scene_id:\n",
    "        scene_dir = os.path.join(program_dir, f'TrimmedTestScene{scene}_graph', 'results_intentions_march-13-18')\n",
    "        # full_object_in_scene = get_all_object_in_scene(data_dir, scene)\n",
    "        for file in os.listdir(scene_dir):\n",
    "            if file.endswith('.txt'):\n",
    "                match = re.search(pattern, file)\n",
    "                if match:\n",
    "                    script_id = match.group(1)\n",
    "                else:\n",
    "                    print(\"Wrong file format. No match found.\")\n",
    "                    continue\n",
    "                if script_id not in t_ids:\n",
    "                    continue\n",
    "                # print(f'{script_id=}')\n",
    "                tot_num += 1\n",
    "                motion_planner, relevant_id, gd_actions, task_name, task_description = construct_planner(name_equivalence, properties_data, object_placing, scenegraph_id=scene, script_id=script_id, dataset_root=data_dir)\n",
    "                node_goals = copy.deepcopy(node_goal_list)\n",
    "                edge_goals = copy.deepcopy(edge_goal_list)\n",
    "\n",
    "                motion_planner.reset()\n",
    "                relevant_nodes = motion_planner.get_relevant_nodes(script_id=script_id)\n",
    "                gd_node_goals, gd_edge_goals = find_node_and_edge_in_scene(node_goals, edge_goals, relevant_nodes, motion_planner)\n",
    "                print(f'{task_name=}')\n",
    "                print(f'{task_description=}')\n",
    "                # print(f'{object_states=}')\n",
    "                if len(gd_node_goals) == 0 and len(gd_edge_goals) == 0 and len(action_goals) == 0:\n",
    "                    return 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "                # relevant obj\n",
    "                object_in_scene, goal_str, relevant_name_to_id = motion_planner.get_goal_describe_nl(task_name, task_description, object_states)\n",
    "\n",
    "                print(f'{goal_str=}')\n",
    "                print(f'{object_in_scene=}')\n",
    "                prompt = open('/Users/bryan/Desktop/wkdir/behavior-vllm-eval/igibson/evaluation/prompts/goal_interpretation.txt', 'r').read()\n",
    "                prompt = prompt.replace('<object_in_scene>', object_in_scene)\n",
    "                prompt = prompt.replace('<goal_str>', goal_str)\n",
    "                prompt = prompt.replace('<relation_types>', str(all_rel))\n",
    "                prompt = prompt.replace('<action_space>', str(action_space))\n",
    "\n",
    "                tot_retry = 3\n",
    "                cur_retry = 0\n",
    "                retry_flag = True\n",
    "                while retry_flag and cur_retry < tot_retry:\n",
    "                    predicted_goals = get_gpt_output(prompt, json_object=True)\n",
    "                    predicted_goals.strip(' ').strip('\\n').strip(' ')\n",
    "                    try:\n",
    "                        predicted_goals = eval(predicted_goals)\n",
    "                        retry_flag = False\n",
    "                        break\n",
    "                    except:\n",
    "                        print('Retry!')\n",
    "                        cur_retry += 1\n",
    "                print(f'{predicted_goals=}')\n",
    "\n",
    "\n",
    "                succ_node_goals, tot_node_goals, succ_edge_goals, tot_edge_goals, succ_action_goals, tot_action_goals = check_goal_interpretation(predicted_goals, gd_node_goals, gd_edge_goals, action_goals, relevant_name_to_id)\n",
    "                tot_nodes += tot_node_goals\n",
    "                tot_edges += tot_edge_goals\n",
    "                tot_actions += tot_action_goals\n",
    "                succ_nodes += succ_node_goals\n",
    "                succ_edges += succ_edge_goals\n",
    "                succ_actions += succ_action_goals\n",
    "                succ_score = (succ_node_goals + succ_edge_goals + succ_action_goals) / (tot_node_goals + tot_edge_goals + tot_action_goals)\n",
    "                assert succ_score >= 0 and succ_score <= 1\n",
    "                tot_succ += succ_score\n",
    "                print(f'{succ_score=}')\n",
    "                print(f'{tot_succ=}')\n",
    "\n",
    "                if not save_case:\n",
    "                    continue\n",
    "                node_score = succ_node_goals/tot_node_goals if tot_node_goals != 0 else -1\n",
    "                edge_score = succ_edge_goals/tot_edge_goals if tot_edge_goals != 0 else -1\n",
    "                if (succ_score <= 0.2 and succ_score >= 0) or (node_score <= 0.2 and node_score >= 0) or (edge_score <= 0.1 and edge_score >= 0):\n",
    "                    # save task, gold goals, predicted goals\n",
    "                    with open(case_path, 'a') as f:\n",
    "                        f.write(f'Script {script_id}\\n')\n",
    "                        f.write(f'Goal type: {task_name}\\n')\n",
    "                        f.write(f'Goal description: {task_description}')\n",
    "                        f.write(f'NODE SCORE={node_score}, {succ_node_goals} out of {tot_node_goals} are correct\\nEDGE SCORE={edge_score}, {succ_edge_goals} out of {tot_edge_goals} are correct\\nTOTAL SCORE={succ_score}, in total {succ_node_goals + succ_edge_goals} out of {tot_node_goals + tot_edge_goals} are correct\\n')\n",
    "                        f.write(f'{object_in_scene=}\\n')\n",
    "                        f.write(f'{goal_str=}\\n')\n",
    "                        f.write(f'Ground truth node goal: {gd_node_goals}\\n')\n",
    "                        f.write(f'Ground truth edge goal: {gd_edge_goals}\\n')\n",
    "                        f.write(f'Ground truth action goal: {action_goals}\\n')\n",
    "                        f.write(f'LLM generated goals: {predicted_goals}\\n')\n",
    "                        f.write('\\n\\n')\n",
    "        \n",
    "    return succ_nodes, tot_nodes, succ_edges, tot_edges, succ_actions, tot_actions, tot_succ, tot_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt_output(message, model=\"gpt-3.5-turbo-0125\", max_tokens=512, temperature=0, json_object=False):\n",
    "    if json_object:\n",
    "        if isinstance(message, str) and not 'json' in message.lower():\n",
    "            message = 'You are a helpful assistant designed to output JSON. ' + message\n",
    "    if openai.__version__.startswith('0.'):\n",
    "        if isinstance(message, str):\n",
    "            messages = [{\"role\": \"user\", \"content\": message}] \n",
    "        else:\n",
    "            messages = message\n",
    "        try:\n",
    "            chat = openai.ChatCompletion.create(\n",
    "                model=model, messages=messages\n",
    "            ) \n",
    "        except Exception as e:\n",
    "            print(f'{e}\\nTry after 1 min')\n",
    "            time.sleep(61)\n",
    "            chat = openai.ChatCompletion.create(\n",
    "                model=model, messages=messages\n",
    "            ) \n",
    "        reply = chat.choices[0].message.content \n",
    "    else:\n",
    "        if isinstance(message, str):\n",
    "            messages = [{\"role\": \"user\", \"content\": message}] \n",
    "        else:\n",
    "            messages = message\n",
    "        kwargs = {\"response_format\": { \"type\": \"json_object\" }} if json_object else {}\n",
    "        try:\n",
    "            chat = openai.OpenAI().chat.completions.create(\n",
    "                messages=messages,\n",
    "                model=model,\n",
    "                temperature=temperature,\n",
    "                **kwargs\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f'{e}\\nTry after 1 min')\n",
    "            time.sleep(61)\n",
    "            chat = openai.OpenAI().chat.completions.create(\n",
    "                messages=messages,\n",
    "                model=model,\n",
    "                temperature=temperature,\n",
    "                **kwargs\n",
    "                )\n",
    "        reply = chat.choices[0].message.content \n",
    "    return reply"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
